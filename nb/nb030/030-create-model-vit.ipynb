{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:18:30.848973Z",
     "iopub.status.busy": "2021-02-06T06:18:30.848275Z",
     "iopub.status.idle": "2021-02-06T06:18:32.004304Z",
     "shell.execute_reply": "2021-02-06T06:18:32.003642Z"
    },
    "papermill": {
     "duration": 1.198477,
     "end_time": "2021-02-06T06:18:32.004456",
     "exception": false,
     "start_time": "2021-02-06T06:18:30.805979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "#TRAIN_PATH = '../input/cassava-leaf-disease-classification/train_images'\n",
    "TRAIN_PATH = '../input/cassava-leaf-disease-merged/train'\n",
    "TEST_PATH = '../input/cassava-leaf-disease-classification/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:18:32.079026Z",
     "iopub.status.busy": "2021-02-06T06:18:32.078337Z",
     "iopub.status.idle": "2021-02-06T06:18:32.085701Z",
     "shell.execute_reply": "2021-02-06T06:18:32.084873Z"
    },
    "papermill": {
     "duration": 0.047297,
     "end_time": "2021-02-06T06:18:32.085834",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.038537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_tfrecords',\n",
       " 'sample_submission.csv',\n",
       " 'test_tfrecords',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_images',\n",
       " 'train.csv',\n",
       " 'test_images']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/cassava-leaf-disease-classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:18:32.166230Z",
     "iopub.status.busy": "2021-02-06T06:18:32.165425Z",
     "iopub.status.idle": "2021-02-06T06:18:32.579979Z",
     "shell.execute_reply": "2021-02-06T06:18:32.580652Z"
    },
    "papermill": {
     "duration": 0.460104,
     "end_time": "2021-02-06T06:18:32.580814",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.120710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  source\n",
       "0  1000015157.jpg      0    2020\n",
       "1  1000201771.jpg      3    2020\n",
       "2   100042118.jpg      1    2020\n",
       "3  1000723321.jpg      1    2020\n",
       "4  1000812911.jpg      3    2020"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/cassava-leaf-disease-merged/merged.csv')\n",
    "test = pd.read_csv('../input/cassava-leaf-disease-classification//sample_submission.csv')\n",
    "label_map = pd.read_json('../input/cassava-leaf-disease-classification/label_num_to_disease_map.json', orient='index')\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035264,
     "end_time": "2021-02-06T06:18:32.651693",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.616429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:18:32.726221Z",
     "iopub.status.busy": "2021-02-06T06:18:32.725523Z",
     "iopub.status.idle": "2021-02-06T06:18:32.730441Z",
     "shell.execute_reply": "2021-02-06T06:18:32.730950Z"
    },
    "papermill": {
     "duration": 0.043992,
     "end_time": "2021-02-06T06:18:32.731133",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.687141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035838,
     "end_time": "2021-02-06T06:18:32.803874",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.768036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:18:32.887138Z",
     "iopub.status.busy": "2021-02-06T06:18:32.883906Z",
     "iopub.status.idle": "2021-02-06T06:18:32.889313Z",
     "shell.execute_reply": "2021-02-06T06:18:32.889808Z"
    },
    "papermill": {
     "duration": 0.050448,
     "end_time": "2021-02-06T06:18:32.889964",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.839516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    apex = False\n",
    "    device = 'TPU' # ['TPU', 'GPU']\n",
    "    nprocs = 8 # [1, 8]\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "    model_name = 'vit_base_patch16_384' # ['deit_base_patch16_224', 'vit_base_patch16_384', 'seresnext50_32x4d', 'tf_efficientnet_b3_ns']\n",
    "    size = 384\n",
    "    freeze_epo = 0 # GradualWarmupSchedulerV2\n",
    "    warmup_epo = 1 # GradualWarmupSchedulerV2\n",
    "    cosine_epo = 9 # GradualWarmupSchedulerV2\n",
    "    epochs = freeze_epo + warmup_epo + cosine_epo # [GradualWarmupSchedulerV2, n_epochs]\n",
    "    scheduler = 'CosineAnnealingWarmRestarts'\n",
    "    loss_train = 'BiTemperedLoss'\n",
    "    epochs = 10\n",
    "    T_0 = 10\n",
    "    lr_1 = 2e-3\n",
    "    lr_2 = 2e-4\n",
    "    t1 = 0.9\n",
    "    t2 = 1.5\n",
    "    smooth = 1e-2\n",
    "    min_lr = 1e-6\n",
    "    batch_size = 32 #[32, 64, 128]\n",
    "    weight_decay = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    rand_augment = False\n",
    "    seed = 42\n",
    "    target_size = 5\n",
    "    target_col = 'label'\n",
    "    n_fold = 5\n",
    "    trn_fold = [0]\n",
    "    train = True\n",
    "    inference = False\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 3\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:18:32.964717Z",
     "iopub.status.busy": "2021-02-06T06:18:32.964060Z",
     "iopub.status.idle": "2021-02-06T06:20:07.747805Z",
     "shell.execute_reply": "2021-02-06T06:20:07.747118Z"
    },
    "papermill": {
     "duration": 94.822357,
     "end_time": "2021-02-06T06:20:07.748005",
     "exception": false,
     "start_time": "2021-02-06T06:18:32.925648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (20.3.1)\r\n",
      "Collecting pip\r\n",
      "  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 14 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 20.3.1\r\n",
      "    Uninstalling pip-20.3.1:\r\n",
      "      Successfully uninstalled pip-20.3.1\r\n",
      "Successfully installed pip-21.0.1\r\n"
     ]
    }
   ],
   "source": [
    "if CFG.device == 'TPU':\n",
    "    import os\n",
    "    os.system('curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py')\n",
    "    os.system('python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev')\n",
    "    os.system('export XLA_USE_BF16=1')\n",
    "    os.system('export XLA_TENSOR_ALLOCATOR_MAXSIZE=100000000')\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    CFG.lr_1 = CFG.lr_1 * CFG.nprocs\n",
    "    CFG.lr_2 = CFG.lr_2 * CFG.nprocs\n",
    "    CFG.batch_size = CFG.batch_size // CFG.nprocs\n",
    "\n",
    "!pip install --upgrade pip\n",
    "if CFG.rand_augment:\n",
    "    !pip install git+https://github.com/ildoonet/pytorch-randaugment > /dev/null\n",
    "    from torchvision.transforms import transforms\n",
    "    from RandAugment import RandAugment\n",
    "    \n",
    "if CFG.scheduler == 'GradualWarmupSchedulerV2': \n",
    "    !pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042804,
     "end_time": "2021-02-06T06:20:07.834016",
     "exception": false,
     "start_time": "2021-02-06T06:20:07.791212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:07.939310Z",
     "iopub.status.busy": "2021-02-06T06:20:07.938603Z",
     "iopub.status.idle": "2021-02-06T06:20:09.914532Z",
     "shell.execute_reply": "2021-02-06T06:20:09.913745Z"
    },
    "papermill": {
     "duration": 2.038191,
     "end_time": "2021-02-06T06:20:09.914724",
     "exception": false,
     "start_time": "2021-02-06T06:20:07.876533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "# from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "sys.path.append('../input/pytorch-sam')\n",
    "from sam import SAM\n",
    "\n",
    "package_path = '../input/image-fmix/FMix-master' #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
    "sys.path.append(package_path)\n",
    "from fmix import sample_mask\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ======================================\n",
    "# Device - TPU or GPU\n",
    "# ======================================\n",
    "if CFG.device == 'TPU':\n",
    "    import ignite.distributed as idist\n",
    "elif CFG.device == 'GPU' and CFG.apex:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.005511Z",
     "iopub.status.busy": "2021-02-06T06:20:10.004506Z",
     "iopub.status.idle": "2021-02-06T06:20:10.007480Z",
     "shell.execute_reply": "2021-02-06T06:20:10.007992Z"
    },
    "papermill": {
     "duration": 0.051223,
     "end_time": "2021-02-06T06:20:10.008285",
     "exception": false,
     "start_time": "2021-02-06T06:20:09.957062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want to use DeiT, please check timm version==0.3.2\n",
    "assert timm.__version__ == \"0.3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04199,
     "end_time": "2021-02-06T06:20:10.092724",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.050734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.192701Z",
     "iopub.status.busy": "2021-02-06T06:20:10.191885Z",
     "iopub.status.idle": "2021-02-06T06:20:10.197137Z",
     "shell.execute_reply": "2021-02-06T06:20:10.196560Z"
    },
    "papermill": {
     "duration": 0.061488,
     "end_time": "2021-02-06T06:20:10.197257",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.135769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f}')\n",
    "    \n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043708,
     "end_time": "2021-02-06T06:20:10.283355",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.239647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.379590Z",
     "iopub.status.busy": "2021-02-06T06:20:10.378823Z",
     "iopub.status.idle": "2021-02-06T06:20:10.405056Z",
     "shell.execute_reply": "2021-02-06T06:20:10.404370Z"
    },
    "papermill": {
     "duration": 0.07958,
     "end_time": "2021-02-06T06:20:10.405202",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.325622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  label\n",
      "0     0         299\n",
      "      1         695\n",
      "      2         604\n",
      "      3        3092\n",
      "      4         578\n",
      "1     0         299\n",
      "      1         695\n",
      "      2         604\n",
      "      3        3092\n",
      "      4         578\n",
      "2     0         298\n",
      "      1         695\n",
      "      2         603\n",
      "      3        3093\n",
      "      4         578\n",
      "3     0         298\n",
      "      1         695\n",
      "      2         603\n",
      "      3        3093\n",
      "      4         578\n",
      "4     0         298\n",
      "      1         696\n",
      "      2         603\n",
      "      3        3092\n",
      "      4         578\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_col])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold', CFG.target_col]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042345,
     "end_time": "2021-02-06T06:20:10.490245",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.447900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.592443Z",
     "iopub.status.busy": "2021-02-06T06:20:10.591726Z",
     "iopub.status.idle": "2021-02-06T06:20:10.595498Z",
     "shell.execute_reply": "2021-02-06T06:20:10.594865Z"
    },
    "papermill": {
     "duration": 0.062358,
     "end_time": "2021-02-06T06:20:10.595680",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.533322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "#         self.labels = pd.get_dummies(df['label']).values  # One Hot Encoding\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        return image, label\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TEST_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.686611Z",
     "iopub.status.busy": "2021-02-06T06:20:10.685622Z",
     "iopub.status.idle": "2021-02-06T06:20:10.689171Z",
     "shell.execute_reply": "2021-02-06T06:20:10.688534Z"
    },
    "papermill": {
     "duration": 0.050772,
     "end_time": "2021-02-06T06:20:10.689303",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.638531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TrainDataset(train, transform=None)\n",
    "\n",
    "# for i in range(1):\n",
    "#     image, label = train_dataset[i]\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(f'label: {label}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042144,
     "end_time": "2021-02-06T06:20:10.773825",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.731681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.880093Z",
     "iopub.status.busy": "2021-02-06T06:20:10.879043Z",
     "iopub.status.idle": "2021-02-06T06:20:10.882247Z",
     "shell.execute_reply": "2021-02-06T06:20:10.881596Z"
    },
    "papermill": {
     "duration": 0.064688,
     "end_time": "2021-02-06T06:20:10.882366",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.817678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(CFG.size, CFG.size), \n",
    "            Transpose(p=0.5), \n",
    "            HorizontalFlip(p=0.5), \n",
    "            VerticalFlip(p=0.5), \n",
    "            ShiftScaleRotate(p=0.5), \n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5), \n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5), \n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "            ), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size), \n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "            ), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "def get_transforms_v2(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop((CFG.size, CFG.size)),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            RandAugment(CFG.N, CFG.M),\n",
    "            transforms.Resize((CFG.size, CFG.size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((CFG.size, CFG.size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:10.973560Z",
     "iopub.status.busy": "2021-02-06T06:20:10.972598Z",
     "iopub.status.idle": "2021-02-06T06:20:10.976122Z",
     "shell.execute_reply": "2021-02-06T06:20:10.975565Z"
    },
    "papermill": {
     "duration": 0.051253,
     "end_time": "2021-02-06T06:20:10.976244",
     "exception": false,
     "start_time": "2021-02-06T06:20:10.924991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TrainDataset(train, transform=get_transforms(data='train'))\n",
    "\n",
    "# for i in range(1):\n",
    "#     image, label = train_dataset[i]\n",
    "#     plt.imshow(image[0])\n",
    "#     plt.title(f'label: {label}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:11.076577Z",
     "iopub.status.busy": "2021-02-06T06:20:11.071016Z",
     "iopub.status.idle": "2021-02-06T06:20:11.117951Z",
     "shell.execute_reply": "2021-02-06T06:20:11.118469Z"
    },
    "papermill": {
     "duration": 0.099674,
     "end_time": "2021-02-06T06:20:11.118649",
     "exception": false,
     "start_time": "2021-02-06T06:20:11.018975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixup(data, target, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "    \n",
    "    lam = np.clip(np.random.beta(alpha, alpha), 0.3, 0.4)\n",
    "    new_data = data.clone()\n",
    "    new_data = lam * new_data[:, :, :, :] + (1 - lam) * data[indices, :, :, :]\n",
    "    targets = (target, shuffled_target, lam)\n",
    "    return new_data, targets\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "    \n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(data, target, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "    \n",
    "    lam = np.clip(np.random.beta(alpha, alpha), 0.3, 0.4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1-((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "    targets = (target, shuffled_target, lam)\n",
    "    return new_data, targets\n",
    "\n",
    "def fmix(data, targets, alpha, decay_power, shape, max_soft=0.0, reformulate=False):\n",
    "    lam, mask = sample_mask(alpha, decay_power, shape, max_soft, reformulate)\n",
    "    #mask =torch.tensor(mask, device=device).float()\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "    x1 = torch.from_numpy(mask).to(device)*data\n",
    "    x2 = torch.from_numpy(1-mask).to(device)*shuffled_data\n",
    "    targets=(targets, shuffled_targets, lam)\n",
    "    \n",
    "    return (x1+x2), targets\n",
    "\n",
    "def get_spm(input,target,model):\n",
    "    imgsize = (CFG.size, CFG.size)\n",
    "    bs = input.size(0)\n",
    "    with torch.no_grad():\n",
    "        output,fms = model(input)\n",
    "        clsw = model.classifier\n",
    "        weight = clsw.weight.data\n",
    "        bias = clsw.bias.data\n",
    "        weight = weight.view(weight.size(0),weight.size(1),1,1)\n",
    "        fms = F.relu(fms)\n",
    "        poolfea = F.adaptive_avg_pool2d(fms,(1,1)).squeeze()\n",
    "        clslogit = F.softmax(clsw.forward(poolfea))\n",
    "        logitlist = []\n",
    "        for i in range(bs):\n",
    "            logitlist.append(clslogit[i,target[i]])\n",
    "        clslogit = torch.stack(logitlist)\n",
    "\n",
    "        out = F.conv2d(fms, weight, bias=bias)\n",
    "\n",
    "        outmaps = []\n",
    "        for i in range(bs):\n",
    "            evimap = out[i,target[i]]\n",
    "            outmaps.append(evimap)\n",
    "\n",
    "        outmaps = torch.stack(outmaps)\n",
    "        if imgsize is not None:\n",
    "            outmaps = outmaps.view(outmaps.size(0),1,outmaps.size(1),outmaps.size(2))\n",
    "            outmaps = F.interpolate(outmaps,imgsize,mode='bilinear',align_corners=False)\n",
    "\n",
    "        outmaps = outmaps.squeeze()\n",
    "\n",
    "        for i in range(bs):\n",
    "            outmaps[i] -= outmaps[i].min()\n",
    "            outmaps[i] /= outmaps[i].sum()\n",
    "\n",
    "\n",
    "    return outmaps,clslogit\n",
    "\n",
    "\n",
    "def snapmix(input, target, alpha, model=None):\n",
    "\n",
    "    r = np.random.rand(1)\n",
    "    lam_a = torch.ones(input.size(0))\n",
    "    lam_b = 1 - lam_a\n",
    "    target_b = target.clone()\n",
    "\n",
    "    if True:\n",
    "        wfmaps,_ = get_spm(input, target, model)\n",
    "        bs = input.size(0)\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam1 = np.random.beta(alpha, alpha)\n",
    "        rand_index = torch.randperm(bs).cuda()\n",
    "        wfmaps_b = wfmaps[rand_index,:,:]\n",
    "        target_b = target[rand_index]\n",
    "\n",
    "        same_label = target == target_b\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "        bbx1_1, bby1_1, bbx2_1, bby2_1 = rand_bbox(input.size(), lam1)\n",
    "\n",
    "        area = (bby2-bby1)*(bbx2-bbx1)\n",
    "        area1 = (bby2_1-bby1_1)*(bbx2_1-bbx1_1)\n",
    "\n",
    "        if  area1 > 0 and area>0:\n",
    "            ncont = input[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()\n",
    "            ncont = F.interpolate(ncont, size=(bbx2-bbx1,bby2-bby1), mode='bilinear', align_corners=True)\n",
    "            input[:, :, bbx1:bbx2, bby1:bby2] = ncont\n",
    "            lam_a = 1 - wfmaps[:,bbx1:bbx2,bby1:bby2].sum(2).sum(1)/(wfmaps.sum(2).sum(1)+1e-8)\n",
    "            lam_b = wfmaps_b[:,bbx1_1:bbx2_1,bby1_1:bby2_1].sum(2).sum(1)/(wfmaps_b.sum(2).sum(1)+1e-8)\n",
    "            tmp = lam_a.clone()\n",
    "            lam_a[same_label] += lam_b[same_label]\n",
    "            lam_b[same_label] += tmp[same_label]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "            lam_a[torch.isnan(lam_a)] = lam\n",
    "            lam_b[torch.isnan(lam_b)] = 1-lam\n",
    "\n",
    "    return input,target,target_b,lam_a.cuda(),lam_b.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041994,
     "end_time": "2021-02-06T06:20:11.203427",
     "exception": false,
     "start_time": "2021-02-06T06:20:11.161433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:11.307964Z",
     "iopub.status.busy": "2021-02-06T06:20:11.306982Z",
     "iopub.status.idle": "2021-02-06T06:20:11.310592Z",
     "shell.execute_reply": "2021-02-06T06:20:11.309925Z"
    },
    "papermill": {
     "duration": 0.064801,
     "end_time": "2021-02-06T06:20:11.310714",
     "exception": false,
     "start_time": "2021-02-06T06:20:11.245913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model_name, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(CFG.model_name, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CustomSEResNeXt(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model_name, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CustomDeiT(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model_name, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('facebookresearch/deit:main', model_name, pretrained=pretrained)\n",
    "        n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model_name, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:11.405505Z",
     "iopub.status.busy": "2021-02-06T06:20:11.404569Z",
     "iopub.status.idle": "2021-02-06T06:20:21.135471Z",
     "shell.execute_reply": "2021-02-06T06:20:21.134829Z"
    },
    "papermill": {
     "duration": 9.782017,
     "end_time": "2021-02-06T06:20:21.135601",
     "exception": false,
     "start_time": "2021-02-06T06:20:11.353584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_p16_384-83fb41ba.pth\n"
     ]
    }
   ],
   "source": [
    "if CFG.model_name=='deit_base_patch16_224':\n",
    "    model = CustomDeiT(model_name=CFG.model_name, pretrained=True)\n",
    "elif CFG.model_name=='vit_base_patch16_384':\n",
    "    model = CustomViT(model_name=CFG.model_name, pretrained=True)\n",
    "elif CFG.model_name=='seresnext50_32x4d':\n",
    "    model = CustomSeResNeXt(CFG.model_name, pretrained=True)\n",
    "elif CFG.model_name=='tf_efficientnet_b3_ns':\n",
    "    model = CustomEfficientNet(CFG.model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.227521Z",
     "iopub.status.busy": "2021-02-06T06:20:21.226550Z",
     "iopub.status.idle": "2021-02-06T06:20:21.231637Z",
     "shell.execute_reply": "2021-02-06T06:20:21.232256Z"
    },
    "papermill": {
     "duration": 0.052978,
     "end_time": "2021-02-06T06:20:21.232419",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.179441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, model_name='tf_efficientnet_b3_ns', pretrained=False):\n",
    "#         super().__init__()\n",
    "#         backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "#         n_features_bn2 = backbone.bn2.num_features\n",
    "#         n_features_classifier = backbone.classifier.in_features\n",
    "#         self.backbone = nn.Sequential(*backbone.children())[:-4]\n",
    "#         self.bn2 = nn.BatchNorm2d(n_features_bn2)\n",
    "#         self.act2 = nn.SiLU(inplace=True)\n",
    "#         self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Linear(n_features_classifier, CFG.target_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         feats = self.backbone(x)\n",
    "#         x = self.bn2(feats)\n",
    "#         x = self.act2(x)\n",
    "#         x = self.global_pool(x).view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.322451Z",
     "iopub.status.busy": "2021-02-06T06:20:21.321473Z",
     "iopub.status.idle": "2021-02-06T06:20:21.325490Z",
     "shell.execute_reply": "2021-02-06T06:20:21.326101Z"
    },
    "papermill": {
     "duration": 0.050916,
     "end_time": "2021-02-06T06:20:21.326253",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.275337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = CustomModel(model_name=CFG.model_name, pretrained=False)\n",
    "# train_dataset = TrainDataset(train, transform=get_transforms(data='train'))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, \n",
    "#                           num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "# for image, label in train_loader:\n",
    "#     print(image.size())\n",
    "#     output = model(image)\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042211,
     "end_time": "2021-02-06T06:20:21.411416",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.369205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.501195Z",
     "iopub.status.busy": "2021-02-06T06:20:21.500200Z",
     "iopub.status.idle": "2021-02-06T06:20:21.510020Z",
     "shell.execute_reply": "2021-02-06T06:20:21.510600Z"
    },
    "papermill": {
     "duration": 0.056464,
     "end_time": "2021-02-06T06:20:21.510735",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.454271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Label Smoothing\n",
    "# ====================================================\n",
    "class LabelSmoothingLoss(nn.Module): \n",
    "    def __init__(self, classes=5, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.600523Z",
     "iopub.status.busy": "2021-02-06T06:20:21.599523Z",
     "iopub.status.idle": "2021-02-06T06:20:21.608194Z",
     "shell.execute_reply": "2021-02-06T06:20:21.608710Z"
    },
    "papermill": {
     "duration": 0.055491,
     "end_time": "2021-02-06T06:20:21.608854",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.553363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.697831Z",
     "iopub.status.busy": "2021-02-06T06:20:21.696844Z",
     "iopub.status.idle": "2021-02-06T06:20:21.707839Z",
     "shell.execute_reply": "2021-02-06T06:20:21.708425Z"
    },
    "papermill": {
     "duration": 0.057849,
     "end_time": "2021-02-06T06:20:21.708591",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.650742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalCosineLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, xent=.1):\n",
    "        super(FocalCosineLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.xent = xent\n",
    "\n",
    "        self.y = torch.Tensor([1]).cuda()\n",
    "\n",
    "    def forward(self, input, target, reduction=\"mean\"):\n",
    "        cosine_loss = F.cosine_embedding_loss(input, F.one_hot(target, num_classes=input.size(-1)), self.y, reduction=reduction)\n",
    "\n",
    "        cent_loss = F.cross_entropy(F.normalize(input), target, reduce=False)\n",
    "        pt = torch.exp(-cent_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * cent_loss\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            focal_loss = torch.mean(focal_loss)\n",
    "\n",
    "        return cosine_loss + self.xent * focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.798628Z",
     "iopub.status.busy": "2021-02-06T06:20:21.797573Z",
     "iopub.status.idle": "2021-02-06T06:20:21.809132Z",
     "shell.execute_reply": "2021-02-06T06:20:21.808551Z"
    },
    "papermill": {
     "duration": 0.057961,
     "end_time": "2021-02-06T06:20:21.809249",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.751288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SymmetricCrossEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=0.1, beta=1.0, num_classes=5):\n",
    "        super(SymmetricCrossEntropy, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets, reduction='mean'):\n",
    "        onehot_targets = torch.eye(self.num_classes)[targets].cuda()\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction=reduction)\n",
    "        rce_loss = (-onehot_targets*logits.softmax(1).clamp(1e-7, 1.0).log()).sum(1)\n",
    "        if reduction == 'mean':\n",
    "            rce_loss = rce_loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            rce_loss = rce_loss.sum()\n",
    "        return self.alpha * ce_loss + self.beta * rce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:21.921659Z",
     "iopub.status.busy": "2021-02-06T06:20:21.905769Z",
     "iopub.status.idle": "2021-02-06T06:20:21.948739Z",
     "shell.execute_reply": "2021-02-06T06:20:21.947968Z"
    },
    "papermill": {
     "duration": 0.096288,
     "end_time": "2021-02-06T06:20:21.948866",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.852578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u'.\"\"\"\n",
    "    if t==1.0:\n",
    "        return u.log()\n",
    "    else:\n",
    "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
    "    if t==1:\n",
    "        return u.exp()\n",
    "    else:\n",
    "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = torch.sum(\n",
    "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "        normalized_activations = normalized_activations_step_0 * \\\n",
    "                logt_partition.pow(1.0-t)\n",
    "\n",
    "    logt_partition = torch.sum(\n",
    "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "    return normalization_constants\n",
    "\n",
    "def compute_normalization_binary_search(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (< 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations = activations - mu\n",
    "\n",
    "    effective_dim = \\\n",
    "        torch.sum(\n",
    "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
    "            dim=-1, keepdim=True).to(activations.dtype)\n",
    "\n",
    "    shape_partition = activations.shape[:-1] + (1,)\n",
    "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
    "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = (upper + lower)/2.0\n",
    "        sum_probs = torch.sum(\n",
    "                exp_t(normalized_activations - logt_partition, t),\n",
    "                dim=-1, keepdim=True)\n",
    "        update = (sum_probs < 1.0).to(activations.dtype)\n",
    "        lower = torch.reshape(\n",
    "                lower * update + (1.0-update) * logt_partition,\n",
    "                shape_partition)\n",
    "        upper = torch.reshape(\n",
    "                upper * (1.0 - update) + update * logt_partition,\n",
    "                shape_partition)\n",
    "\n",
    "    logt_partition = (upper + lower)/2.0\n",
    "    return logt_partition + mu\n",
    "\n",
    "class ComputeNormalization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, activations, t, num_iters):\n",
    "        if t < 1.0:\n",
    "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
    "        else:\n",
    "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "        ctx.save_for_backward(activations, normalization_constants)\n",
    "        ctx.t=t\n",
    "        return normalization_constants\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        activations, normalization_constants = ctx.saved_tensors\n",
    "        t = ctx.t\n",
    "        normalized_activations = activations - normalization_constants \n",
    "        probabilities = exp_t(normalized_activations, t)\n",
    "        escorts = probabilities.pow(t)\n",
    "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
    "        grad_input = escorts * grad_output\n",
    "        \n",
    "        return grad_input, None, None\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example. \n",
    "    Backward pass is implemented.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    return ComputeNormalization.apply(activations, t, num_iters)\n",
    "\n",
    "def tempered_sigmoid(activations, t, num_iters = 5):\n",
    "    \"\"\"Tempered sigmoid function.\n",
    "    Args:\n",
    "      activations: Activations for the positive class for binary classification.\n",
    "      t: Temperature tensor > 0.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
    "    return internal_probabilities[..., 0]\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature > 1.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    if t == 1.0:\n",
    "        return activations.softmax(dim=-1)\n",
    "\n",
    "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "def bi_tempered_binary_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing = 0.0,\n",
    "        num_iters=5,\n",
    "        reduction='mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered binary logistic loss.\n",
    "    Args:\n",
    "      activations: A tensor containing activations for class 1.\n",
    "      labels: A tensor with shape as activations, containing probabilities for class 1\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_labels = torch.stack([labels.to(activations.dtype),\n",
    "        1.0 - labels.to(activations.dtype)],\n",
    "        dim=-1)\n",
    "    return bi_tempered_logistic_loss(internal_activations, \n",
    "            internal_labels,\n",
    "            t1,\n",
    "            t2,\n",
    "            label_smoothing = label_smoothing,\n",
    "            num_iters = num_iters,\n",
    "            reduction = reduction)\n",
    "\n",
    "def bi_tempered_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing=0.0,\n",
    "        num_iters=5,\n",
    "        reduction = 'mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      labels: A tensor with shape and dtype as activations (onehot), \n",
    "        or a long tensor of one dimension less than activations (pytorch standard)\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
    "      num_iters: Number of iterations to run the method. Default 5.\n",
    "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
    "        ``'none'``: No reduction is applied, return shape is shape of\n",
    "        activations without the last dimension.\n",
    "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
    "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
    "        labels_onehot = torch.zeros_like(activations)\n",
    "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "    else:\n",
    "        labels_onehot = labels\n",
    "\n",
    "    if label_smoothing > 0:\n",
    "        num_classes = labels_onehot.shape[-1]\n",
    "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
    "                * labels_onehot + \\\n",
    "                label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
    "            - labels_onehot * log_t(probabilities, t1) \\\n",
    "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
    "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
    "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss_values\n",
    "    if reduction == 'sum':\n",
    "        return loss_values.sum()\n",
    "    if reduction == 'mean':\n",
    "        return loss_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.043984Z",
     "iopub.status.busy": "2021-02-06T06:20:22.043293Z",
     "iopub.status.idle": "2021-02-06T06:20:22.046620Z",
     "shell.execute_reply": "2021-02-06T06:20:22.046089Z"
    },
    "papermill": {
     "duration": 0.054937,
     "end_time": "2021-02-06T06:20:22.046739",
     "exception": false,
     "start_time": "2021-02-06T06:20:21.991802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiTemperedLogisticLoss(nn.Module): \n",
    "    def __init__(self, t1, t2, smoothing=0.0): \n",
    "        super(BiTemperedLogisticLoss, self).__init__() \n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, logit_label, truth_label):\n",
    "        loss_label = bi_tempered_logistic_loss(\n",
    "            logit_label, truth_label,\n",
    "            t1=self.t1, t2=self.t2,\n",
    "            label_smoothing=self.smoothing,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        loss_label = loss_label.mean()\n",
    "        return loss_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.149353Z",
     "iopub.status.busy": "2021-02-06T06:20:22.148406Z",
     "iopub.status.idle": "2021-02-06T06:20:22.151334Z",
     "shell.execute_reply": "2021-02-06T06:20:22.150819Z"
    },
    "papermill": {
     "duration": 0.061342,
     "end_time": "2021-02-06T06:20:22.151470",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.090128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaylorSoftmax(nn.Module):\n",
    "    '''\n",
    "    This is the autograd version\n",
    "    '''\n",
    "    def __init__(self, dim=1, n=2):\n",
    "        super(TaylorSoftmax, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        usage similar to nn.Softmax:\n",
    "            >>> mod = TaylorSoftmax(dim=1, n=4)\n",
    "            >>> inten = torch.randn(1, 32, 64, 64)\n",
    "            >>> out = mod(inten)\n",
    "        '''\n",
    "        fn = torch.ones_like(x)\n",
    "        denor = 1.\n",
    "        for i in range(1, self.n+1):\n",
    "            denor *= i\n",
    "            fn = fn + x.pow(i) / denor\n",
    "        out = fn / fn.sum(dim=self.dim, keepdims=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TaylorCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, n=2, ignore_index=-1, reduction='mean', smoothing=0.05):\n",
    "        super(TaylorCrossEntropyLoss, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lab_smooth = LabelSmoothingLoss(CFG.target_size, smoothing=smoothing)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        log_probs = self.taylor_softmax(logits).log()\n",
    "        loss = self.lab_smooth(log_probs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.246611Z",
     "iopub.status.busy": "2021-02-06T06:20:22.245697Z",
     "iopub.status.idle": "2021-02-06T06:20:22.248617Z",
     "shell.execute_reply": "2021-02-06T06:20:22.248050Z"
    },
    "papermill": {
     "duration": 0.053716,
     "end_time": "2021-02-06T06:20:22.248730",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.195014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SnapMixLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, criterion, outputs, ya, yb, lam_a, lam_b):\n",
    "        loss_a = criterion(outputs, ya)\n",
    "        loss_b = criterion(outputs, yb)\n",
    "        loss = torch.mean(loss_a * lam_a + loss_b * lam_b)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.340439Z",
     "iopub.status.busy": "2021-02-06T06:20:22.339729Z",
     "iopub.status.idle": "2021-02-06T06:20:22.344841Z",
     "shell.execute_reply": "2021-02-06T06:20:22.344214Z"
    },
    "papermill": {
     "duration": 0.053116,
     "end_time": "2021-02-06T06:20:22.344959",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.291843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "#     def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "#         super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "#     def get_lr(self):\n",
    "#         if self.last_epoch > self.total_epoch:\n",
    "#             if self.after_scheduler:\n",
    "#                 if not self.finished:\n",
    "#                     self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "#                     self.finished = True\n",
    "#                 return self.after_scheduler.get_lr()\n",
    "#             return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "#         if self.multiplier == 1.0:\n",
    "#             return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "#         else:\n",
    "#             return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.472259Z",
     "iopub.status.busy": "2021-02-06T06:20:22.451296Z",
     "iopub.status.idle": "2021-02-06T06:20:22.486451Z",
     "shell.execute_reply": "2021-02-06T06:20:22.485846Z"
    },
    "papermill": {
     "duration": 0.098374,
     "end_time": "2021-02-06T06:20:22.486579",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.388205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def asMinutes(s):\n",
    "    \"\"\"秒を分に変換する関数\"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"経過時間の測定と終了時間の予測を行う関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    since : float\n",
    "        実験を始めた時刻\n",
    "    percent : float\n",
    "        実験が進んだ割合\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    s : 経過時間\n",
    "    re : 終了までの時間の予測\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    s = now - since  # 経過時間の測定\n",
    "    es = s / percent  # 終了時間の予測\n",
    "    re = es - s  # 残り時間の予想\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(re))\n",
    "\n",
    "def train_fn(train_loader, model, loss_train, loss_metric, optimizer, epoch, shechduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if CFG.device == 'GPU':\n",
    "            \n",
    "#             def closure():\n",
    "#                 loss = loss_train(model(images), labels)\n",
    "#                 loss.backward()\n",
    "#                 return loss\n",
    "                \n",
    "            y_preds = model(images)\n",
    "            metric = loss_metric(y_preds, labels)\n",
    "            loss = loss_train(y_preds, labels)\n",
    "            # record loss\n",
    "            losses.update(metric.item(), batch_size)\n",
    "            if CFG.gradient_accumulation_steps > 1:\n",
    "                loss = loss / CFG.gradient_accumulation_steps\n",
    "            if CFG.apex:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else: \n",
    "                loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "#             # measure elapsed time\n",
    "#             loss_train(model(images), labels).backward()\n",
    "#             optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}]'\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})'\n",
    "                      'Elapsed {remain:s}' \n",
    "                      'Loss: {loss.val:.4f}({loss.avg:.4f})' \n",
    "                      'Grad: {grad_norm:.4f}  '\n",
    "                      .format(epoch+1, step, len(train_loader), batch_time=batch_time, \n",
    "                              data_time=data_time, loss=losses, \n",
    "                              remain=timeSince(start, float(step+1)/len(train_loader)), \n",
    "                              grad_norm=grad_norm))\n",
    "                    \n",
    "        elif CFG.device == 'TPU':\n",
    "            \n",
    "            def closure():\n",
    "                loss = loss_train(model(images), labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            \n",
    "            y_preds = model(images)\n",
    "            metric = loss_metric(y_preds, labels)\n",
    "            loss = loss_train(y_preds, labels)\n",
    "            # record loss\n",
    "            losses.update(metric.item(), batch_size)\n",
    "            if CFG.gradient_accumulation_steps > 1:\n",
    "                loss = loss / CFG.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                xm.optimizer_step(optimizer)\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "#             loss_train(model(images), labels).backward()\n",
    "#             optimizer.second_step(zero_grad=True)\n",
    "            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "                xm.master_print('Epoch: [{0}][{1}/{2}] '\n",
    "                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                                'Elapsed {remain:s} '\n",
    "                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                                'Grad: {grad_norm:.4f}  '\n",
    "                                #'LR: {lr:.6f}  '\n",
    "                                .format(\n",
    "                                epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses,\n",
    "                                remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                                grad_norm=grad_norm,\n",
    "                                #lr=scheduler.get_lr()[0],\n",
    "                                ))\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, loss_metric, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    trues = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        loss = loss_metric(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        trues.append(labels.to('cpu').numpy())\n",
    "        preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if CFG.device == 'GPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                      .format(\n",
    "                       step, len(valid_loader), batch_time=batch_time,\n",
    "                       data_time=data_time, loss=losses,\n",
    "                       remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                       ))\n",
    "        elif CFG.device == 'TPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "                xm.master_print('EVAL: [{0}/{1}] '\n",
    "                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                                'Elapsed {remain:s} '\n",
    "                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                                .format(\n",
    "                                step, len(valid_loader), batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses,\n",
    "                                remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                                ))\n",
    "    trues = np.concatenate(trues)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions, trues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04332,
     "end_time": "2021-02-06T06:20:22.572825",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.529505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.721828Z",
     "iopub.status.busy": "2021-02-06T06:20:22.694968Z",
     "iopub.status.idle": "2021-02-06T06:20:22.725019Z",
     "shell.execute_reply": "2021-02-06T06:20:22.724384Z"
    },
    "papermill": {
     "duration": 0.108936,
     "end_time": "2021-02-06T06:20:22.725157",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.616221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Train loop\n",
    "# ======================================================\n",
    "\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    seed_torch(seed=CFG.seed)    \n",
    "    \n",
    "    xm.master_print(f'========== fold: {fold} training ============')\n",
    "    \n",
    "    # ======================================================\n",
    "    # loader\n",
    "    # ======================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    \n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    if CFG.rand_augment:\n",
    "        train_dataset = TrainDataset_v2(train_folds, \n",
    "                                 transform=get_transforms_v2(data='train'))\n",
    "        valid_dataset = TrainDataset_v2(valid_folds, \n",
    "                                 transform=get_transforms_v2(data='valid'))\n",
    "    else:\n",
    "        train_dataset = TrainDataset(train_folds, \n",
    "                                 transform=get_transforms(data='train'))\n",
    "        valid_dataset = TrainDataset(valid_folds, \n",
    "                                 transform=get_transforms(data='valid'))\n",
    "        \n",
    "    train_sampler = DistributedSampler(train_dataset, \n",
    "                                       num_replicas=xm.xrt_world_size(), \n",
    "                                       rank=xm.get_ordinal(), \n",
    "                                       shuffle=True)\n",
    "    valid_sampler = DistributedSampler(valid_dataset, \n",
    "                                       num_replicas=xm.xrt_world_size(), \n",
    "                                       rank=xm.get_ordinal(), \n",
    "                                       shuffle=False)\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              sampler=train_sampler, \n",
    "                              num_workers=CFG.num_workers, \n",
    "                              pin_memory=True, \n",
    "                              drop_last=False)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              sampler=valid_sampler, \n",
    "                              num_workers=CFG.num_workers, \n",
    "                              pin_memory=True, \n",
    "                              drop_last=False)\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "    \n",
    "    # ===============================================\n",
    "    # scheduler\n",
    "    # ===============================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='GradualWarmupSchedulerV2':\n",
    "            scheduler_cosine=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, CFG.cosine_epo)\n",
    "            scheduler_warmup=GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=CFG.warmup_epo, after_scheduler=scheduler_cosine)\n",
    "            scheduler=scheduler_warmup\n",
    "        return scheduler\n",
    "    \n",
    "    # ===============================================\n",
    "    # model & optimizer\n",
    "    # ===============================================\n",
    "    if CFG.device == 'TPU':\n",
    "        device = xm.xla_device()\n",
    "        xm.set_rng_state(CFG.seed, device)\n",
    "    elif CFG.device == 'GPU':\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def get_model(pretrained=False):\n",
    "        if CFG.model_name=='deit_base_patch16_224':\n",
    "            model = CustomDeiT(model_name=CFG.model_name, pretrained=pretrained)\n",
    "        elif CFG.model_name=='vit_base_patch16_384':\n",
    "            model = CustomViT(model_name=CFG.model_name, pretrained=pretrained)\n",
    "        elif CFG.model_name=='seresnext50_32x4d':\n",
    "            model = CustomSeResNeXt(CFG.model_name, pretrained=pretrained)\n",
    "        elif CFG.model_name=='tf_efficientnet_b3_ns':\n",
    "            model = CustomEfficientNet(CFG.model_name, pretrained=pretrained)\n",
    "        return model\n",
    "        \n",
    "    model = get_model(pretrained=True)\n",
    "    model.to(device)\n",
    "    \n",
    "#     # 最初の1epochは全結合層以外全て凍結する。\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'head' not in name:\n",
    "#             param.requires_grad=False\n",
    "    \n",
    "#     base_optimizer = Adam\n",
    "    optimizer = Adam(model.parameters(), lr=CFG.lr_2, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    \n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    \n",
    "    # ===============================================\n",
    "    # apex \n",
    "    # ===============================================\n",
    "    if CFG.apex:\n",
    "        model.optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "        \n",
    "    # ===============================================\n",
    "    # loop\n",
    "    # ===============================================\n",
    "    def get_loss_train():\n",
    "        if CFG.loss_train == 'CrossEntropyLoss':\n",
    "            loss_train = nn.CrossEntropyLoss()\n",
    "        elif CFG.loss_train == 'LabelSmoothing':\n",
    "            loss_train = LabelSmoothingLoss(classes=CFG.target_size, smoothing=CFG.smooth)\n",
    "        elif CFG.loss_train == 'FocalLoss':\n",
    "            loss_train = FocalLoss().to(device)\n",
    "        elif CFG.loss_train == 'FocalCosineLoss':\n",
    "            loss_train = FocalCosineLoss()\n",
    "        elif CFG.loss_train == 'SymmetricCrossEntropyLoss':\n",
    "            loss_train = SymmetricCrossEntropy().to(device)\n",
    "        elif CFG.loss_train == 'BiTemperedLoss':\n",
    "            loss_train = BiTemperedLogisticLoss(t1=CFG.t1, t2=CFG.t2, smoothing=CFG.smooth)\n",
    "        elif CFG.criterion=='TaylorCrossEntropyLoss':\n",
    "            criterion = TaylorCrossEntropyLoss(smoothing=CFG.smoothing)\n",
    "        return loss_train\n",
    "    \n",
    "    loss_train = get_loss_train()\n",
    "    loss_metric = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "#         if epoch == 1:\n",
    "            \n",
    "#             # 2epoch目に重みを全て解凍する\n",
    "#             for param in model.parameters():\n",
    "#                 param.requires_grad = True\n",
    "                \n",
    "#             # 学習率を落とす\n",
    "# #             base_optimizer = Adam\n",
    "#             optimizer = Adam(model.parameters(), lr=CFG.lr_2, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "#             scheduler = get_scheduler(optimizer)\n",
    "        \n",
    "        # train\n",
    "        if CFG.device == 'TPU':\n",
    "            \n",
    "            if CFG.nprocs == 1:\n",
    "                avg_loss = train_fn(train_loader, model, loss_train, loss_metric, optimizer, epoch, scheduler, device)\n",
    "            elif CFG.nprocs == 8:\n",
    "                para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "                avg_loss = train_fn(para_train_loader.per_device_loader(device), model, loss_train, loss_metric, optimizer, epoch, scheduler, device)\n",
    "        \n",
    "        elif CFG.device == 'GPU':\n",
    "            avg_loss = train_fn(train_loader, model, loss_train, loss_metric, optimizer, epoch, scheduler, device)\n",
    "        \n",
    "        # eval\n",
    "        if CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                avg_val_loss, preds, _ = valid_fn(valid_loader, model, loss_metric, device)\n",
    "            elif CFG.nprocs == 8:\n",
    "                para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
    "                avg_val_loss, preds, valid_labels = valid_fn(para_valid_loader.per_device_loader(device), model, loss_metric, device)\n",
    "                preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n",
    "                valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n",
    "        elif CFG.device == 'GPU':\n",
    "            avg_val_loss, preds, _ = valid_fn(valid_loader, model, loss_metric, device)\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "        \n",
    "        # scoring\n",
    "        score = get_score(valid_labels, preds.argmax(1))\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "            elif CFG.nprocs == 8:\n",
    "                xm.master_print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "                xm.master_print(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "                \n",
    "        elif CFG.device == 'GPU':\n",
    "            LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            \n",
    "            if CFG.device == 'TPU':\n",
    "                if CFG.nprocs == 1:\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                elif CFG.nprocs == 8:\n",
    "                    xm.master_print(f'Epoch: {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                xm.save({'model': model.state_dict(), \n",
    "                         'preds': preds}, \n",
    "                         OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            elif CFG.device == 'GPU':\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(), \n",
    "                            'preds': preds}, \n",
    "                            OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            \n",
    "            \n",
    "        # inference用に全て保存しておく\n",
    "        if CFG.device == 'TPU':\n",
    "            xm.save({'model': model.state_dict()}, OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_epoch{epoch+1}.pth')\n",
    "        elif CFG.device == 'GPU':\n",
    "            torch.save({'model': model.state_dict()}, OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_epoch{epoch+1}.pth')\n",
    "    \n",
    "    if CFG.nprocs != 8:\n",
    "        check_point = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "        valid_folds[[str(c) for c in range(5)]] = check_point['preds']\n",
    "        valid_folds['preds'] = check_point['preds'].argmax(1)\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.815093Z",
     "iopub.status.busy": "2021-02-06T06:20:22.814434Z",
     "iopub.status.idle": "2021-02-06T06:20:22.823851Z",
     "shell.execute_reply": "2021-02-06T06:20:22.824491Z"
    },
    "papermill": {
     "duration": 0.056101,
     "end_time": "2021-02-06T06:20:22.824669",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.768568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df[CFG.target_col].values\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.5f}')\n",
    "        \n",
    "    if CFG.train:\n",
    "        # train\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(folds, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                if CFG.nprocs != 8:\n",
    "                    LOGGER.info(f'=============== fold: {fold} result ================')\n",
    "                    get_result(_oof_df)\n",
    "        \n",
    "        if CFG.nprocs != 8:\n",
    "            # CV result\n",
    "            LOGGER.info(f'============ CV ============')\n",
    "            get_result(oof_df)\n",
    "            # save result\n",
    "            oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-06T06:20:22.916137Z",
     "iopub.status.busy": "2021-02-06T06:20:22.915444Z",
     "iopub.status.idle": "2021-02-06T06:20:26.297817Z",
     "shell.execute_reply": "2021-02-06T06:20:26.297238Z"
    },
    "papermill": {
     "duration": 3.428718,
     "end_time": "2021-02-06T06:20:26.297963",
     "exception": false,
     "start_time": "2021-02-06T06:20:22.869245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ProcessExitedException",
     "evalue": "process 3 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0617f87afd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    380\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_xla_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# If this is not an XLA setup, jump to normal multi-processing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_run_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0mpf_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_fork_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36m_run_direct\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     return torch.multiprocessing.spawn(\n\u001b[0;32m--> 346\u001b[0;31m         fn, args=args, nprocs=nprocs, join=join, daemon=daemon)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    228\u001b[0m                ' torch.multiprocessing.start_process(...)' % start_method)\n\u001b[1;32m    229\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 )\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 3 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if CFG.device == 'TPU':\n",
    "        def _mp_fn(rank, flags):\n",
    "            torch.set_default_tensor_type('torch.FloatTensor')\n",
    "            a = main()\n",
    "        FLAGS = {}\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=CFG.nprocs, start_method='fork')\n",
    "    elif CFG.device == 'GPU':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044284,
     "end_time": "2021-02-06T06:20:26.386688",
     "exception": false,
     "start_time": "2021-02-06T06:20:26.342404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.043435,
     "end_time": "2021-02-06T06:20:26.473891",
     "exception": false,
     "start_time": "2021-02-06T06:20:26.430456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.043088,
     "end_time": "2021-02-06T06:20:26.560323",
     "exception": false,
     "start_time": "2021-02-06T06:20:26.517235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 120.985394,
   "end_time": "2021-02-06T06:20:26.712537",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-06T06:18:25.727143",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
