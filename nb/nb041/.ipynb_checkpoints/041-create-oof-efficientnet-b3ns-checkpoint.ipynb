{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:31.641552Z",
     "iopub.status.busy": "2021-01-10T23:44:31.640708Z",
     "iopub.status.idle": "2021-01-10T23:44:32.651354Z",
     "shell.execute_reply": "2021-01-10T23:44:32.650663Z"
    },
    "papermill": {
     "duration": 1.044844,
     "end_time": "2021-01-10T23:44:32.651483",
     "exception": false,
     "start_time": "2021-01-10T23:44:31.606639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:32.712588Z",
     "iopub.status.busy": "2021-01-10T23:44:32.711616Z",
     "iopub.status.idle": "2021-01-10T23:44:32.718997Z",
     "shell.execute_reply": "2021-01-10T23:44:32.718469Z"
    },
    "papermill": {
     "duration": 0.038676,
     "end_time": "2021-01-10T23:44:32.719091",
     "exception": false,
     "start_time": "2021-01-10T23:44:32.680415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_tfrecords',\n",
       " 'sample_submission.csv',\n",
       " 'test_tfrecords',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_images',\n",
       " 'train.csv',\n",
       " 'test_images']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/home/yuki/kaggle/input/cassava-leaf-disease-classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:32.786446Z",
     "iopub.status.busy": "2021-01-10T23:44:32.78566Z",
     "iopub.status.idle": "2021-01-10T23:44:33.177494Z",
     "shell.execute_reply": "2021-01-10T23:44:33.177946Z"
    },
    "papermill": {
     "duration": 0.430582,
     "end_time": "2021-01-10T23:44:33.178087",
     "exception": false,
     "start_time": "2021-01-10T23:44:32.747505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  source\n",
       "0  1000015157.jpg      0    2020\n",
       "1  1000201771.jpg      3    2020\n",
       "2   100042118.jpg      1    2020\n",
       "3  1000723321.jpg      1    2020\n",
       "4  1000812911.jpg      3    2020"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv('/home/yuki/kaggle/input/cassava-leaf-disease-merged/merged.csv')\n",
    "test = pd.read_csv('/home/yuki/kaggle/input/cassava-leaf-disease-classification//sample_submission.csv')\n",
    "label_map = pd.read_json('/home/yuki/kaggle/input/cassava-leaf-disease-classification/label_num_to_disease_map.json', orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029264,
     "end_time": "2021-01-10T23:44:33.238047",
     "exception": false,
     "start_time": "2021-01-10T23:44:33.208783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:33.308046Z",
     "iopub.status.busy": "2021-01-10T23:44:33.307186Z",
     "iopub.status.idle": "2021-01-10T23:44:33.309815Z",
     "shell.execute_reply": "2021-01-10T23:44:33.310307Z"
    },
    "papermill": {
     "duration": 0.040693,
     "end_time": "2021-01-10T23:44:33.310457",
     "exception": false,
     "start_time": "2021-01-10T23:44:33.269764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "TRAIN_PATH = '/home/yuki/kaggle/input/cassava-leaf-disease-merged/train'\n",
    "TEST_PATH = '/home/yuki/kaggle/input/cassava-leaf-disease-classification/test_images'\n",
    "MODEL_DIR = '/home/yuki/kaggle/input/nb023-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030811,
     "end_time": "2021-01-10T23:44:33.371498",
     "exception": false,
     "start_time": "2021-01-10T23:44:33.340687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:33.434987Z",
     "iopub.status.busy": "2021-01-10T23:44:33.434075Z",
     "iopub.status.idle": "2021-01-10T23:44:33.44434Z",
     "shell.execute_reply": "2021-01-10T23:44:33.443802Z"
    },
    "papermill": {
     "duration": 0.042772,
     "end_time": "2021-01-10T23:44:33.444444",
     "exception": false,
     "start_time": "2021-01-10T23:44:33.401672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    apex = False\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "    model_name = 'tf_efficientnet_b3_ns'\n",
    "    size = 470\n",
    "    scheduler = 'CosineAnnealingWarmRestarts'\n",
    "    loss_train = 'BiTemperedLoss'\n",
    "    epochs = [7, 8, 9, 10]\n",
    "    T_0 = 10\n",
    "    lr_1 = 5e-4\n",
    "    lr_2 = 5e-5\n",
    "    t1 = 0.9\n",
    "    t2 = 1.5\n",
    "    smooth = 1e-2\n",
    "    min_lr = 1e-6\n",
    "    batch_size = 128\n",
    "    weight_decay = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    target_size = 5\n",
    "    target_col = 'label'\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    tta = 5\n",
    "    train = True\n",
    "    inference = False\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = [9, 10]\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029859,
     "end_time": "2021-01-10T23:44:33.505353",
     "exception": false,
     "start_time": "2021-01-10T23:44:33.475494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:33.581634Z",
     "iopub.status.busy": "2021-01-10T23:44:33.578694Z",
     "iopub.status.idle": "2021-01-10T23:44:38.183416Z",
     "shell.execute_reply": "2021-01-10T23:44:38.182864Z"
    },
    "papermill": {
     "duration": 4.6479,
     "end_time": "2021-01-10T23:44:38.183538",
     "exception": false,
     "start_time": "2021-01-10T23:44:33.535638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yuki/kaggle/input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "sys.path.append('/home/yuki/kaggle/input/pytorch-sam')\n",
    "from sam import SAM\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if CFG.apex:\n",
    "    from apex import amp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029927,
     "end_time": "2021-01-10T23:44:38.244935",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.215008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:38.319565Z",
     "iopub.status.busy": "2021-01-10T23:44:38.318811Z",
     "iopub.status.idle": "2021-01-10T23:44:38.326217Z",
     "shell.execute_reply": "2021-01-10T23:44:38.325686Z"
    },
    "papermill": {
     "duration": 0.05071,
     "end_time": "2021-01-10T23:44:38.326321",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.275611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f}')\n",
    "    \n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029185,
     "end_time": "2021-01-10T23:44:38.385415",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.35623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:38.49042Z",
     "iopub.status.busy": "2021-01-10T23:44:38.488289Z",
     "iopub.status.idle": "2021-01-10T23:44:38.514416Z",
     "shell.execute_reply": "2021-01-10T23:44:38.513637Z"
    },
    "papermill": {
     "duration": 0.063697,
     "end_time": "2021-01-10T23:44:38.514539",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.450842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  label\n",
      "0     0         13\n",
      "      1         23\n",
      "      2         23\n",
      "      3        117\n",
      "      4         24\n",
      "1     0         13\n",
      "      1         23\n",
      "      2         23\n",
      "      3        116\n",
      "      4         25\n",
      "2     0         12\n",
      "      1         23\n",
      "      2         24\n",
      "      3        116\n",
      "      4         25\n",
      "3     0         12\n",
      "      1         24\n",
      "      2         24\n",
      "      3        116\n",
      "      4         24\n",
      "4     0         12\n",
      "      1         24\n",
      "      2         24\n",
      "      3        116\n",
      "      4         24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_col])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold', CFG.target_col]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038208,
     "end_time": "2021-01-10T23:44:38.586003",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.547795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:38.674723Z",
     "iopub.status.busy": "2021-01-10T23:44:38.673893Z",
     "iopub.status.idle": "2021-01-10T23:44:38.677502Z",
     "shell.execute_reply": "2021-01-10T23:44:38.677998Z"
    },
    "papermill": {
     "duration": 0.056762,
     "end_time": "2021-01-10T23:44:38.678153",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.621391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "#         self.labels = pd.get_dummies(df['label']).values  # One Hot Encoding\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        return image, label\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:38.749252Z",
     "iopub.status.busy": "2021-01-10T23:44:38.747379Z",
     "iopub.status.idle": "2021-01-10T23:44:38.749927Z",
     "shell.execute_reply": "2021-01-10T23:44:38.750488Z"
    },
    "papermill": {
     "duration": 0.040421,
     "end_time": "2021-01-10T23:44:38.750616",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.710195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TrainDataset(train, transform=None)\n",
    "\n",
    "# for i in range(1):\n",
    "#     image, label = train_dataset[i]\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(f'label: {label}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032394,
     "end_time": "2021-01-10T23:44:38.814655",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.782261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:38.887553Z",
     "iopub.status.busy": "2021-01-10T23:44:38.886744Z",
     "iopub.status.idle": "2021-01-10T23:44:38.890944Z",
     "shell.execute_reply": "2021-01-10T23:44:38.890443Z"
    },
    "papermill": {
     "duration": 0.045439,
     "end_time": "2021-01-10T23:44:38.891044",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.845605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(CFG.size, CFG.size), \n",
    "            Transpose(p=0.5), \n",
    "            HorizontalFlip(p=0.5), \n",
    "            VerticalFlip(p=0.5), \n",
    "            ShiftScaleRotate(p=0.5), \n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5), \n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5), \n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "            ), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size), \n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "            ), \n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'test':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(CFG.size, CFG.size), \n",
    "#             Transpose(p=0.5), \n",
    "#             HorizontalFlip(p=0.5), \n",
    "#             VerticalFlip(p=0.5), \n",
    "#             ShiftScaleRotate(p=0.5), \n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "            \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:38.959604Z",
     "iopub.status.busy": "2021-01-10T23:44:38.958866Z",
     "iopub.status.idle": "2021-01-10T23:44:38.963225Z",
     "shell.execute_reply": "2021-01-10T23:44:38.962697Z"
    },
    "papermill": {
     "duration": 0.040755,
     "end_time": "2021-01-10T23:44:38.963336",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.922581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TrainDataset(train, transform=get_transforms(data='train'))\n",
    "\n",
    "# for i in range(1):\n",
    "#     image, label = train_dataset[i]\n",
    "#     plt.imshow(image[0])\n",
    "#     plt.title(f'label: {label}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030495,
     "end_time": "2021-01-10T23:44:39.024817",
     "exception": false,
     "start_time": "2021-01-10T23:44:38.994322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.093385Z",
     "iopub.status.busy": "2021-01-10T23:44:39.092644Z",
     "iopub.status.idle": "2021-01-10T23:44:39.095606Z",
     "shell.execute_reply": "2021-01-10T23:44:39.095104Z"
    },
    "papermill": {
     "duration": 0.040068,
     "end_time": "2021-01-10T23:44:39.095703",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.055635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomEfficientNetB3ns(nn.Module):\n",
    "    def __init__(self, model_name='tf_efficientnet_b3_ns', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, CFG.target_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.159945Z",
     "iopub.status.busy": "2021-01-10T23:44:39.159098Z",
     "iopub.status.idle": "2021-01-10T23:44:39.161536Z",
     "shell.execute_reply": "2021-01-10T23:44:39.162068Z"
    },
    "papermill": {
     "duration": 0.036979,
     "end_time": "2021-01-10T23:44:39.1622",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.125221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = CustomEfficientNetB3ns(model_name=CFG.model_name, pretrained=False)\n",
    "# train_dataset = TrainDataset(train, transform=get_transforms(data='train'))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, \n",
    "#                           num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "# for image, label in train_loader:\n",
    "#     print(image.size())\n",
    "#     output = model(image)\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030285,
     "end_time": "2021-01-10T23:44:39.223653",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.193368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.300889Z",
     "iopub.status.busy": "2021-01-10T23:44:39.298857Z",
     "iopub.status.idle": "2021-01-10T23:44:39.301732Z",
     "shell.execute_reply": "2021-01-10T23:44:39.302303Z"
    },
    "papermill": {
     "duration": 0.047777,
     "end_time": "2021-01-10T23:44:39.302441",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.254664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Label Smoothing\n",
    "# ====================================================\n",
    "class LabelSmoothingLoss(nn.Module): \n",
    "    def __init__(self, classes=5, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.379992Z",
     "iopub.status.busy": "2021-01-10T23:44:39.378337Z",
     "iopub.status.idle": "2021-01-10T23:44:39.380853Z",
     "shell.execute_reply": "2021-01-10T23:44:39.381366Z"
    },
    "papermill": {
     "duration": 0.045285,
     "end_time": "2021-01-10T23:44:39.381485",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.3362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.459437Z",
     "iopub.status.busy": "2021-01-10T23:44:39.458536Z",
     "iopub.status.idle": "2021-01-10T23:44:39.461831Z",
     "shell.execute_reply": "2021-01-10T23:44:39.462336Z"
    },
    "papermill": {
     "duration": 0.048797,
     "end_time": "2021-01-10T23:44:39.462472",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.413675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalCosineLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, xent=.1):\n",
    "        super(FocalCosineLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.xent = xent\n",
    "\n",
    "        self.y = torch.Tensor([1]).cuda()\n",
    "\n",
    "    def forward(self, input, target, reduction=\"mean\"):\n",
    "        cosine_loss = F.cosine_embedding_loss(input, F.one_hot(target, num_classes=input.size(-1)), self.y, reduction=reduction)\n",
    "\n",
    "        cent_loss = F.cross_entropy(F.normalize(input), target, reduce=False)\n",
    "        pt = torch.exp(-cent_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * cent_loss\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            focal_loss = torch.mean(focal_loss)\n",
    "\n",
    "        return cosine_loss + self.xent * focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.541908Z",
     "iopub.status.busy": "2021-01-10T23:44:39.538439Z",
     "iopub.status.idle": "2021-01-10T23:44:39.544964Z",
     "shell.execute_reply": "2021-01-10T23:44:39.544351Z"
    },
    "papermill": {
     "duration": 0.048331,
     "end_time": "2021-01-10T23:44:39.545069",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.496738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SymmetricCrossEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=0.1, beta=1.0, num_classes=5):\n",
    "        super(SymmetricCrossEntropy, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets, reduction='mean'):\n",
    "        onehot_targets = torch.eye(self.num_classes)[targets].cuda()\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction=reduction)\n",
    "        rce_loss = (-onehot_targets*logits.softmax(1).clamp(1e-7, 1.0).log()).sum(1)\n",
    "        if reduction == 'mean':\n",
    "            rce_loss = rce_loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            rce_loss = rce_loss.sum()\n",
    "        return self.alpha * ce_loss + self.beta * rce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.638927Z",
     "iopub.status.busy": "2021-01-10T23:44:39.61807Z",
     "iopub.status.idle": "2021-01-10T23:44:39.662083Z",
     "shell.execute_reply": "2021-01-10T23:44:39.661516Z"
    },
    "papermill": {
     "duration": 0.084023,
     "end_time": "2021-01-10T23:44:39.662221",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.578198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u'.\"\"\"\n",
    "    if t==1.0:\n",
    "        return u.log()\n",
    "    else:\n",
    "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
    "    if t==1:\n",
    "        return u.exp()\n",
    "    else:\n",
    "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = torch.sum(\n",
    "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "        normalized_activations = normalized_activations_step_0 * \\\n",
    "                logt_partition.pow(1.0-t)\n",
    "\n",
    "    logt_partition = torch.sum(\n",
    "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "    return normalization_constants\n",
    "\n",
    "def compute_normalization_binary_search(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (< 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations = activations - mu\n",
    "\n",
    "    effective_dim = \\\n",
    "        torch.sum(\n",
    "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
    "            dim=-1, keepdim=True).to(activations.dtype)\n",
    "\n",
    "    shape_partition = activations.shape[:-1] + (1,)\n",
    "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
    "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = (upper + lower)/2.0\n",
    "        sum_probs = torch.sum(\n",
    "                exp_t(normalized_activations - logt_partition, t),\n",
    "                dim=-1, keepdim=True)\n",
    "        update = (sum_probs < 1.0).to(activations.dtype)\n",
    "        lower = torch.reshape(\n",
    "                lower * update + (1.0-update) * logt_partition,\n",
    "                shape_partition)\n",
    "        upper = torch.reshape(\n",
    "                upper * (1.0 - update) + update * logt_partition,\n",
    "                shape_partition)\n",
    "\n",
    "    logt_partition = (upper + lower)/2.0\n",
    "    return logt_partition + mu\n",
    "\n",
    "class ComputeNormalization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, activations, t, num_iters):\n",
    "        if t < 1.0:\n",
    "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
    "        else:\n",
    "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "        ctx.save_for_backward(activations, normalization_constants)\n",
    "        ctx.t=t\n",
    "        return normalization_constants\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        activations, normalization_constants = ctx.saved_tensors\n",
    "        t = ctx.t\n",
    "        normalized_activations = activations - normalization_constants \n",
    "        probabilities = exp_t(normalized_activations, t)\n",
    "        escorts = probabilities.pow(t)\n",
    "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
    "        grad_input = escorts * grad_output\n",
    "        \n",
    "        return grad_input, None, None\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example. \n",
    "    Backward pass is implemented.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    return ComputeNormalization.apply(activations, t, num_iters)\n",
    "\n",
    "def tempered_sigmoid(activations, t, num_iters = 5):\n",
    "    \"\"\"Tempered sigmoid function.\n",
    "    Args:\n",
    "      activations: Activations for the positive class for binary classification.\n",
    "      t: Temperature tensor > 0.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
    "    return internal_probabilities[..., 0]\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature > 1.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    if t == 1.0:\n",
    "        return activations.softmax(dim=-1)\n",
    "\n",
    "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "def bi_tempered_binary_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing = 0.0,\n",
    "        num_iters=5,\n",
    "        reduction='mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered binary logistic loss.\n",
    "    Args:\n",
    "      activations: A tensor containing activations for class 1.\n",
    "      labels: A tensor with shape as activations, containing probabilities for class 1\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_labels = torch.stack([labels.to(activations.dtype),\n",
    "        1.0 - labels.to(activations.dtype)],\n",
    "        dim=-1)\n",
    "    return bi_tempered_logistic_loss(internal_activations, \n",
    "            internal_labels,\n",
    "            t1,\n",
    "            t2,\n",
    "            label_smoothing = label_smoothing,\n",
    "            num_iters = num_iters,\n",
    "            reduction = reduction)\n",
    "\n",
    "def bi_tempered_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing=0.0,\n",
    "        num_iters=5,\n",
    "        reduction = 'mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      labels: A tensor with shape and dtype as activations (onehot), \n",
    "        or a long tensor of one dimension less than activations (pytorch standard)\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
    "      num_iters: Number of iterations to run the method. Default 5.\n",
    "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
    "        ``'none'``: No reduction is applied, return shape is shape of\n",
    "        activations without the last dimension.\n",
    "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
    "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
    "        labels_onehot = torch.zeros_like(activations)\n",
    "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "    else:\n",
    "        labels_onehot = labels\n",
    "\n",
    "    if label_smoothing > 0:\n",
    "        num_classes = labels_onehot.shape[-1]\n",
    "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
    "                * labels_onehot + \\\n",
    "                label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
    "            - labels_onehot * log_t(probabilities, t1) \\\n",
    "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
    "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
    "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss_values\n",
    "    if reduction == 'sum':\n",
    "        return loss_values.sum()\n",
    "    if reduction == 'mean':\n",
    "        return loss_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.734228Z",
     "iopub.status.busy": "2021-01-10T23:44:39.733565Z",
     "iopub.status.idle": "2021-01-10T23:44:39.738064Z",
     "shell.execute_reply": "2021-01-10T23:44:39.737323Z"
    },
    "papermill": {
     "duration": 0.043696,
     "end_time": "2021-01-10T23:44:39.738195",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.694499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiTemperedLogisticLoss(nn.Module): \n",
    "    def __init__(self, t1, t2, smoothing=0.0): \n",
    "        super(BiTemperedLogisticLoss, self).__init__() \n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, logit_label, truth_label):\n",
    "        loss_label = bi_tempered_logistic_loss(\n",
    "            logit_label, truth_label,\n",
    "            t1=self.t1, t2=self.t2,\n",
    "            label_smoothing=self.smoothing,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        loss_label = loss_label.mean()\n",
    "        return loss_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:39.827487Z",
     "iopub.status.busy": "2021-01-10T23:44:39.816863Z",
     "iopub.status.idle": "2021-01-10T23:44:39.851715Z",
     "shell.execute_reply": "2021-01-10T23:44:39.85114Z"
    },
    "papermill": {
     "duration": 0.08097,
     "end_time": "2021-01-10T23:44:39.85182",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.77085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def asMinutes(s):\n",
    "    \"\"\"秒を分に変換する関数\"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"経過時間の測定と終了時間の予測を行う関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    since : float\n",
    "        実験を始めた時刻\n",
    "    percent : float\n",
    "        実験が進んだ割合\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    s : 経過時間\n",
    "    re : 終了までの時間の予測\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    s = now - since  # 経過時間の測定\n",
    "    es = s / percent  # 終了時間の予測\n",
    "    re = es - s  # 残り時間の予想\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(re))\n",
    "\n",
    "def train_fn(train_loader, model, loss_train, loss_metric, optimizer, epoch, shechduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(images)\n",
    "        metric = loss_metric(y_preds, labels)\n",
    "        loss = loss_train(y_preds, labels)\n",
    "        # record loss\n",
    "        losses.update(metric.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if CFG.apex:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else: \n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "#             optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        loss_train(model(images), labels).backward()\n",
    "#         loss = torch.mean(loss)\n",
    "#         loss.backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}]'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})'\n",
    "                  'Elapsed {remain:s}' \n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f})' \n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), batch_time=batch_time, \n",
    "                          data_time=data_time, loss=losses, \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)), \n",
    "                          grad_norm=grad_norm))\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, loss_metric, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        loss = loss_metric(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "            \n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "def inference(model, states, test_loader, device):\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    probs = []\n",
    "    for i, (images) in tk0:\n",
    "        images = images.to(device)\n",
    "        avg_preds = []\n",
    "        for state in states:\n",
    "            model.load_state_dict(state['model'])\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images)\n",
    "            avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        avg_preds = np.mean(avg_preds, axis=0)\n",
    "        probs.append(avg_preds)\n",
    "    probs = np.concatenate(probs)\n",
    "    return probs\n",
    "\n",
    "def inference_with_tta(model, states, test_loader, device):\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    probs = []\n",
    "    for i, (images) in tk0:\n",
    "        x = images.to(device)\n",
    "        x = torch.stack([x,x.flip(-1),x.flip(-2),x.flip(-1,-2),\n",
    "            x.transpose(-1,-2),x.transpose(-1,-2).flip(-1),\n",
    "            x.transpose(-1,-2).flip(-2),x.transpose(-1,-2).flip(-1,-2)],0)\n",
    "        x = x.view(-1, 3, CFG.size, CFG.size)\n",
    "        avg_preds = []\n",
    "        for state in states:\n",
    "            model.load_state_dict(state['model'])\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(x)\n",
    "                y_preds = y_preds.view(CFG.batch_size, 8, -1).mean(axis=1)\n",
    "            avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        avg_preds = np.mean(avg_preds, axis=0)\n",
    "        probs.append(avg_preds)\n",
    "    probs = np.concatenate(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03065,
     "end_time": "2021-01-10T23:44:39.914309",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.883659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:40.019948Z",
     "iopub.status.busy": "2021-01-10T23:44:40.004717Z",
     "iopub.status.idle": "2021-01-10T23:44:40.023102Z",
     "shell.execute_reply": "2021-01-10T23:44:40.022555Z"
    },
    "papermill": {
     "duration": 0.077083,
     "end_time": "2021-01-10T23:44:40.023235",
     "exception": false,
     "start_time": "2021-01-10T23:44:39.946152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Train loop\n",
    "# ======================================================\n",
    "\n",
    "def train_loop(folds, fold, epoch):\n",
    "    \n",
    "    seed_torch(seed=CFG.seed)    \n",
    "    \n",
    "#     LOGGER.info(f'========== fold: {fold} training ============')\n",
    "    \n",
    "    # ======================================================\n",
    "    # loader\n",
    "    # ======================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    \n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "#     train_dataset = TestDataset(train_folds, \n",
    "#                                 transform=get_transforms(data='train'))\n",
    "    valid_dataset = TestDataset(valid_folds, \n",
    "                                transform=get_transforms(data='test'))\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, \n",
    "#                               batch_size=CFG.batch_size, \n",
    "#                               shuffle=True, \n",
    "#                               num_workers=CFG.num_workers, \n",
    "#                               pin_memory=True, \n",
    "#                               drop_last=False)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, \n",
    "                              pin_memory=True, \n",
    "                              drop_last=False)\n",
    "    \n",
    "    \n",
    "    model = CustomEfficientNetB3ns(CFG.model_name, pretrained=True)\n",
    "    states = [torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_best.pth')]\n",
    "        \n",
    "    start_time = time.time()\n",
    "\n",
    "    # eval\n",
    "    preds = inference(model, states, valid_loader, device)\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    # scoring\n",
    "    score = get_score(valid_labels, preds.argmax(1))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "#     LOGGER.info(f'Epoch {epoch+1} - time: {elapsed:.0f}s Accuracy: {score}')\n",
    "\n",
    "    valid_folds[[str(c) for c in range(5)]] = preds\n",
    "    valid_folds['preds'] = preds.argmax(1)\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:40.101848Z",
     "iopub.status.busy": "2021-01-10T23:44:40.10094Z",
     "iopub.status.idle": "2021-01-10T23:44:40.103416Z",
     "shell.execute_reply": "2021-01-10T23:44:40.103942Z"
    },
    "papermill": {
     "duration": 0.048162,
     "end_time": "2021-01-10T23:44:40.104065",
     "exception": false,
     "start_time": "2021-01-10T23:44:40.055903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare: 1.train 2.test 3.submission 4.folds\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df[CFG.target_col].values\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.5f}')\n",
    "        \n",
    "    if CFG.train:\n",
    "        # train\n",
    "        oof_epoch = []\n",
    "        for epoch in CFG.epochs:\n",
    "            LOGGER.info(f'epoch: {epoch}')\n",
    "            oof_tta = []\n",
    "            for tta in range(CFG.tta):\n",
    "                LOGGER.info(f'=============== tta: {tta} ===============')\n",
    "                oof_fold = []\n",
    "                for fold in range(CFG.n_fold):\n",
    "                    if fold in CFG.trn_fold:\n",
    "                        _oof_df = train_loop(folds, fold, epoch)\n",
    "                        _oof_array = _oof_df[['0', '1', '2', '3', '4']].values\n",
    "                        oof_fold.append(_oof_array)\n",
    "                        LOGGER.info(f'=============== fold: {fold} result ================')\n",
    "                        get_result(_oof_df)\n",
    "                oof_tta.append(np.concatenate(oof_fold, axis=0))\n",
    "            oof_epoch.append(np.mean(oof_tta, axis=0))\n",
    "            \n",
    "                \n",
    "            # CV result\n",
    "#             LOGGER.info(f'============ CV ============')\n",
    "#             get_result(oof_df)\n",
    "            # save result\n",
    "#             oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n",
    "#             oof.append(oof_df)\n",
    "        for i, oof in enumerate(oof_epoch):\n",
    "            np.save(OUTPUT_DIR+f'oof_{i+7}.npy', oof)\n",
    "        \n",
    "\n",
    "    if CFG.inference:\n",
    "        # inference\n",
    "        model = CustomEfficientNetB3ns(CFG.model_name, pretrained=False)\n",
    "        states = [torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth') for fold in CFG.trn_fold]\n",
    "        test_dataset = TestDataset(test, batch_size=CFG.batch_size, shuffle=False, pin_memory=True)\n",
    "        predictions = inference(model, states, test_loader, device)\n",
    "        # submission\n",
    "        test['label'] = predictions.argmax(1)\n",
    "        test[['image_id', 'label']].to_csv(OUTPUT_DIR+'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:40.171602Z",
     "iopub.status.busy": "2021-01-10T23:44:40.170625Z",
     "iopub.status.idle": "2021-01-10T23:44:40.175383Z",
     "shell.execute_reply": "2021-01-10T23:44:40.174852Z"
    },
    "papermill": {
     "duration": 0.039968,
     "end_time": "2021-01-10T23:44:40.175517",
     "exception": false,
     "start_time": "2021-01-10T23:44:40.135549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "used device: cuda\n"
     ]
    }
   ],
   "source": [
    "LOGGER.info(f'used device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T23:44:40.246713Z",
     "iopub.status.busy": "2021-01-10T23:44:40.245678Z",
     "iopub.status.idle": "2021-01-11T03:33:32.065685Z",
     "shell.execute_reply": "2021-01-11T03:33:32.065097Z"
    },
    "papermill": {
     "duration": 13731.857505,
     "end_time": "2021-01-11T03:33:32.065808",
     "exception": false,
     "start_time": "2021-01-10T23:44:40.208303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "=============== tta: 0 ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be832f31ad3a41db8f3f0e687b55cb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 0 result ================\n",
      "Score: 0.91000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2ac02336ce4f2ea963ff7700da06c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 1 result ================\n",
      "Score: 0.93500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4449cdc8ed6441a2b817bb4fb1c8d41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 2 result ================\n",
      "Score: 0.91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf07be7215434bbc873fc78caec4d071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 3 result ================\n",
      "Score: 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5196192738f84095b01e13629efec179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 4 result ================\n",
      "Score: 0.86500\n",
      "=============== tta: 1 ===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19247a6c3fe45c099529044929611c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 0 result ================\n",
      "Score: 0.91000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1c9ed3835047c294b89447d001cd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 1 result ================\n",
      "Score: 0.93500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529d69266437435f8953145a44332969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 2 result ================\n",
      "Score: 0.91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83793e06bf342bdb64990d2e3cd3336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 3 result ================\n",
      "Score: 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d911ee241c954fa4abb026043bfe1046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 4 result ================\n",
      "Score: 0.86500\n",
      "epoch: 10\n",
      "=============== tta: 0 ===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105eb3cd56cb42688d2820bcfcbb1712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 0 result ================\n",
      "Score: 0.91000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee18826dacc04db8ad6dd6be36ba4bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 1 result ================\n",
      "Score: 0.93500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ecd0d62649450ea719687fe50c90db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 2 result ================\n",
      "Score: 0.91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be541cf0951450a83212cb849aad879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 3 result ================\n",
      "Score: 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992c3d15c11143a3be3645927f333d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 4 result ================\n",
      "Score: 0.86500\n",
      "=============== tta: 1 ===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48625b36a3aa400da151441a97d4048e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 0 result ================\n",
      "Score: 0.91000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d81b8d6fef944fbb2118d00a9f0bb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 1 result ================\n",
      "Score: 0.93500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca85b3405184796bc90cf22473590ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 2 result ================\n",
      "Score: 0.91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537e4158219241fc916ae1823447b76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 3 result ================\n",
      "Score: 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9496149e7a4c72ac48de519fea7854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============== fold: 4 result ================\n",
      "Score: 0.86500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
